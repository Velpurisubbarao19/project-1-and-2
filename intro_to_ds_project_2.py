# -*- coding: utf-8 -*-
"""intro to ds project 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVBysVlX5cj5XOvCZBgcqpeQ9XxRoRj_
"""

# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('telco-customer-churn.csv')

# Check for missing values
print("Missing values per column:")
print(df.isnull().sum())

# Replace empty spaces with NaN for consistency
df.replace(" ", np.nan, inplace=True)

# Check for outliers using boxplots
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
for col in numeric_cols:
    plt.figure(figsize=(10, 5))
    sns.boxplot(x=df[col])
    plt.title(f'Outliers in {col}')
    plt.show()

# Convert categorical columns to numeric using LabelEncoder
label_encoder = LabelEncoder()
for col in df.select_dtypes(include=['object']).columns:
    df[col] = label_encoder.fit_transform(df[col])

# Scale numeric features
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Check final dataset info
df.info()

# Display data types
print("Data Types:")
print(df.dtypes.value_counts())
print("\nCategorical Columns:", df.select_dtypes(include=['object']).columns.tolist())
print("Numeric Columns:", df.select_dtypes(include=['float64', 'int64']).columns.tolist())

# Correlation matrix to check dependencies
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation")
plt.show()

# Feature importance with Random Forest
from sklearn.ensemble import RandomForestClassifier
X = df.drop('Churn', axis=1)
y = df['Churn']
rf = RandomForestClassifier()
rf.fit(X, y)
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})
feature_importance.sort_values(by='Importance', ascending=False, inplace=True)
print("Top Important Features:")
print(feature_importance.head(10))

# Plot feature importance
plt.figure(figsize=(10, 5))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))
plt.title("Top 10 Feature Importance")
plt.show()

import sweetviz as sv

# Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Use SweetViz to compare datasets
train_test_compare = sv.compare([X_train, "Training Data"], [X_test, "Test Data"])
train_test_compare.show_html('train_test_comparison.html')

# Dataset limitations
print("Limitations and Issues:")
print("1. The dataset may have class imbalance as the churn rate could be lower compared to non-churners.")
print("2. Feature scaling might be needed for models sensitive to feature magnitudes.")
print("3. SMOTE (Synthetic Minority Over-sampling Technique) will need to be applied to balance the churn classes.")

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# Load dataset
data = pd.read_csv('telco-customer-churn.csv')  # Replace with your file path

# Preprocess dataset
data['Churn'] = data['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)
data = pd.get_dummies(data)
X = data.drop('Churn', axis=1)
y = data['Churn']

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the dataset
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define function for model evaluation
def evaluate_model(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")

# Naive Bayes Model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)
print("\nNaive Bayes Performance:")
evaluate_model(y_test, y_pred_nb)

# Logistic Regression Model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)
print("\nLogistic Regression Performance:")
evaluate_model(y_test, y_pred_lr)

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
print("\nRandom Forest Performance:")
evaluate_model(y_test, y_pred_rf)

# XGBoost Model
xgb_model = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgboost = xgb_model.predict(X_test)
print("\nXGBoost Performance:")
evaluate_model(y_test, y_pred_xgboost)

# Define function for model evaluation
def evaluate_model(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")

# Original Dataset - Model Training and Evaluation
print("Original Dataset Performance:")

# Naive Bayes Model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)
print("\nNaive Bayes Performance:")
evaluate_model(y_test, y_pred_nb)

# Logistic Regression Model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)
print("\nLogistic Regression Performance:")
evaluate_model(y_test, y_pred_lr)

# Random Forest Model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
print("\nRandom Forest Performance:")
evaluate_model(y_test, y_pred_rf)

# XGBoost Model
xgb_model = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgboost = xgb_model.predict(X_test)
print("\nXGBoost Performance:")
evaluate_model(y_test, y_pred_xgboost)

# Apply SMOTE to address data imbalance
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# SMOTE Dataset - Model Training and Evaluation
print("\nSMOTE Dataset Performance:")

# Naive Bayes Model with SMOTE
nb_model.fit(X_train_smote, y_train_smote)
y_pred_nb_smote = nb_model.predict(X_test)
print("\nNaive Bayes Performance (SMOTE):")
evaluate_model(y_test, y_pred_nb_smote)

# Logistic Regression Model with SMOTE
lr_model.fit(X_train_smote, y_train_smote)
y_pred_lr_smote = lr_model.predict(X_test)
print("\nLogistic Regression Performance (SMOTE):")
evaluate_model(y_test, y_pred_lr_smote)

# Random Forest Model with SMOTE
rf_model.fit(X_train_smote, y_train_smote)
y_pred_rf_smote = rf_model.predict(X_test)
print("\nRandom Forest Performance (SMOTE):")
evaluate_model(y_test, y_pred_rf_smote)

# XGBoost Model with SMOTE
xgb_model.fit(X_train_smote, y_train_smote)
y_pred_xgboost_smote = xgb_model.predict(X_test)
print("\nXGBoost Performance (SMOTE):")
evaluate_model(y_test, y_pred_xgboost_smote)

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Define models
models = {
    'Naive Bayes': GaussianNB(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assuming X_train and y_train are your training features and labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit models
for name, model in models.items():
    model.fit(X_train, y_train)
    print(f"{name} model trained.")

# Evaluate models
for name, model in models.items():
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print(f"{name} Model Evaluation:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}\n")

from sklearn.model_selection import GridSearchCV

# Set up hyperparameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize Random Forest Classifier
rf_model = RandomForestClassifier()

# Set up GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=3, n_jobs=-1, verbose=2, scoring='recall')

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Best parameters and best score
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Recall Score: {grid_search.best_score_:.4f}")



